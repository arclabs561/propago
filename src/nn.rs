use candle_core::{Result, Tensor};
use candle_nn::{Linear, Module};

/// Graph Convolutional Network Layer
pub struct GCNConv {
    lin: Linear,
}

impl GCNConv {
    /// Construct a GCN layer from a Candle `Linear` projection.
    pub fn new(lin: Linear) -> Self {
        Self { lin }
    }

    pub fn forward(&self, x: &Tensor, adj: &Tensor) -> Result<Tensor> {
        // Basic GCN logic placeholder
        // A_hat * X * W
        let x = self.lin.forward(x)?;
        adj.matmul(&x)
    }
}

/// Graph Attention Network Layer
#[doc(hidden)]
#[deprecated(
    note = "GATConv is not implemented yet; prefer GCNConv or HGCNConv for now."
)]
pub struct GATConv {
    #[allow(dead_code)] // placeholder layer; forward path not wired yet
    lin: Linear,
    // att: Linear...
}

#[allow(deprecated)]
impl GATConv {
    /// Construct a placeholder GAT layer.
    ///
    /// Note: attention weights are not implemented yet.
    pub fn new(lin: Linear) -> Self {
        Self { lin }
    }

    /// Placeholder forward: currently just applies the linear projection.
    pub fn forward(&self, x: &Tensor, _adj: &Tensor) -> Result<Tensor> {
        self.lin.forward(x)
    }
}
